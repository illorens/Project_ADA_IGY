{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is healthy food for rich people? A data story about nutrition and wealth\n",
    "\n",
    "Our aim is to explore the relations that exist between the social and economic status of the French population and the quality of the food that is available. \n",
    "\n",
    "Indeed, we start from a point where we believe there is a correlation between obesity and socio-economic level: poor people tend to be more overweight than their richer counterparts. Moreover, obesity has been linked with the consumption of food items that have high rates of sugar, fat and energy.\n",
    "\n",
    "We wish to explore a possible *intermediate link* between obesity and socio-economic level: the quality of the *available* food. \n",
    "\n",
    "Our dataframe includes labelled products, their nutritional information, a few nutritional scores and the cities where these products are sold. We have coupled this data with economical and social data from French cities in order to obtain a clean dataframe containing for each row:\n",
    "- a product identifier\n",
    "- its nutritional information\n",
    "- the city where it is sold (if sold in several cities, the row is duplicated)\n",
    "- the median revenue, mean revenue, poverty rate and other economic features relative to the city\n",
    "\n",
    "In order to perform our analysis, we have proceeded in several steps:\n",
    "\n",
    "1. Exploration and data cleaning\n",
    "\n",
    "2. Exploration of the distribution of the nutrition grades at each geographical level for the richest and poorest zone\n",
    "\n",
    "3. Distribution of the products frequency according to their nutrition grade\n",
    "\n",
    "4. Implementation of two additional custom nutrition scores\n",
    "\n",
    "5. Dimensionality reduction\n",
    "\n",
    "6. Data aggregation according to a certain geographical level (city, arrondissement, department or region)\n",
    "\n",
    "7. Attempt at clustering\n",
    "\n",
    "8. Correlation exploration\n",
    "\n",
    "9. Geographic visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init(os.environ['SPARK_HOME'])\n",
    "\n",
    "import folium\n",
    "from IPython.core.display import display, HTML\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import re\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from termcolor import colored\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import (GeoJSONDataSource, HoverTool, LogColorMapper, LinearColorMapper, BasicTicker,  \n",
    "                            PrintfTickFormatter, ColorBar)\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.palettes import Viridis256, OrRd\n",
    "from bokeh.transform import linear_cmap\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "import random\n",
    "from branca.colormap import LinearColormap\n",
    "from folium.folium import color_brewer, StepColormap, GeoJson, TopoJson\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploration and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first handle the nutrition-related data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts = spark.read.csv('../data/raw/en.openfoodfacts.org.products.csv', header=True, sep=\"\\t\")\n",
    "food_facts.registerTempTable('food_facts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. First exploration of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_products = food_facts.count()\n",
    "print(\"We have {} products in our dataframe.\".format(number_of_products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of distinct city tags without france: {}\".format(\n",
    "    food_facts.select('cities_tags').filter('cities_tags NOT LIKE \"%france%\"')\n",
    "    .distinct().count())\n",
    ")\n",
    "print(\"Number of distinct city tags : {}\".format(\n",
    "    food_facts.select('cities_tags').distinct().count())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of distinct purchase places without france in the name: {}\".format(\n",
    "    food_facts.select('purchase_places').filter('LOWER(purchase_places) NOT LIKE \"%france%\"')\n",
    "    .distinct().count()))\n",
    "print(\"Number of distinct purchase places : {}\".format(\n",
    "    food_facts.select('purchase_places').distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of manufacturing places without france in the name: {}\".format(\n",
    "    food_facts.select('manufacturing_places_tags')\n",
    "              .filter('LOWER(manufacturing_places_tags) NOT LIKE \"%france%\"')\n",
    "              .distinct().count())\n",
    ")\n",
    "print(\"Number of manufacturing places : {}\".format(\n",
    "    food_facts.select('manufacturing_places_tags').distinct().count())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems more interesting to make an analysis focused on the French situation rather than the situation in the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts.select('generic_name').filter('generic_name IS NOT NULL') \\\n",
    "    .distinct().toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll probably have to filter out the names that are not present in French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nutrition_grades = food_facts.select(\n",
    "    'nutrition_grade_fr', 'nutrition_grade_uk', 'code', 'nutrition-score-fr_100g', 'nutrition-score-uk_100g'\n",
    "    ).filter('nutrition_grade_fr IS NOT NULL OR nutrition_grade_uk IS NOT NULL OR `nutrition-score-uk_100g` IS NOT NULL OR `nutrition-score-fr_100g` IS NOT NULL').toPandas()\n",
    "print(\"\"\"Total number of products for which we have nutrition informations: \n",
    "{}\"\"\"\n",
    "      .format(nutrition_grades.count())\n",
    ")\n",
    "print(\"Proportion of products for which we have nutrition informations: {ratio:.3%}\"\n",
    "      .format(ratio=(nutrition_grades['code'].count()/number_of_products))\n",
    ")\n",
    "print(\"Description of the table: \")\n",
    "nutrition_grades.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `nutrition_grade_uk` is useless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Food*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select only the columns we plan to use\n",
    "food_dataframe = food_facts.select('generic_name',\n",
    "                                   'code',\n",
    "                                   'nutrition_grade_fr',\n",
    "                                   'nutrition-score-fr_100g',\n",
    "                                   'nutrition-score-uk_100g',\n",
    "                                   'serving_size',\n",
    "                                   'energy_100g',\n",
    "                                   'energy-from-fat_100g', \n",
    "                                   'trans-fat_100g',\n",
    "                                   'fat_100g',\n",
    "                                   'saturated-fat_100g',\n",
    "                                   'monounsaturated-fat_100g',\n",
    "                                   'polyunsaturated-fat_100g',\n",
    "                                   'cholesterol_100g',\n",
    "                                   'proteins_100g',\n",
    "                                   'carbohydrates_100g', \n",
    "                                   'sugars_100g', \n",
    "                                   'fiber_100g',\n",
    "                                   'fruits-vegetables-nuts_100g',\n",
    "                                   'fruits-vegetables-nuts-estimate_100g',\n",
    "                                   'glycemic-index_100g',\n",
    "                                   'cities',\n",
    "                                   'cities_tags',\n",
    "                                   'purchase_places',\n",
    "                                   'stores',\n",
    "                                   'countries',\n",
    "                                   'countries_tags')\n",
    "# move to pandas for the rest of the analysis\n",
    "food_dataframe = food_dataframe.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first remove all products that are not sold in France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_dataframe = food_dataframe[np.logical_not(food_dataframe['countries_tags'].apply(\n",
    "                                        lambda x: x is None or \"france\" not in x.lower())\n",
    "               & food_dataframe['countries'].apply(lambda x: x is None or \"france\" not in x.lower())\n",
    "               & food_dataframe['cities_tags'].apply(lambda x: x is None or \"france\" not in x.lower())\n",
    "               & food_dataframe['cities'].apply(lambda x: x is None or \"france\" not in x.lower())\n",
    "               & food_dataframe['purchase_places'].apply(lambda x: x is None or \"france\" not in x.lower())\n",
    "               & food_dataframe['stores'].apply(lambda x: x is None or \"france\" not in x.lower()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to check how many of our features have a significant amount of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_rows = len(food_dataframe)\n",
    "for col in food_dataframe.columns:\n",
    "    none_values = np.count_nonzero(food_dataframe[col].apply(lambda x: x is None))\n",
    "    not_none_percentage = (total_rows - none_values) / total_rows\n",
    "    print('Rows that are **not** None in {col}: {p:.3%}'.format(col=col, p=not_none_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on this, we will have to remove some rows:**\n",
    "\n",
    "We remove the rows for which we have less than 10% of not None values.\n",
    "\n",
    "- energy-from-fat_100g\n",
    "- monounsaturated-fat_100g\n",
    "- polyunsaturated-fat_100g\n",
    "- cholesterol_100g\n",
    "- trans-fat_100g\n",
    "- fruits-vegetables-nuts_100g\n",
    "- fruits-vegetables-nuts-estimate_100g\n",
    "- glycemic-index_100g\n",
    "- cities\n",
    "- generic_name\n",
    "\n",
    "Since we have the code of the product, we drop the generic_name attribute too.\n",
    "\n",
    "We also need to drop all rows for which we have no information whatsoever (nutrition_grade, fat, saturated fat, sugars, proteins, fiber, carbohydrates, energy).\n",
    "\n",
    "Finally, we need to drop all rows for which we have no information on the city where it is sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns with too few values\n",
    "food_dataframe = food_dataframe.drop(columns=['energy-from-fat_100g', \n",
    "                             'monounsaturated-fat_100g', \n",
    "                             'polyunsaturated-fat_100g',\n",
    "                             'cholesterol_100g',\n",
    "                             'trans-fat_100g',\n",
    "                             'fruits-vegetables-nuts_100g',\n",
    "                             'fruits-vegetables-nuts-estimate_100g',\n",
    "                             'glycemic-index_100g',\n",
    "                             'glycemic-index_100g',\n",
    "                             'cities',\n",
    "                             'generic_name'])\n",
    "\n",
    "# Drop the rows with too little nutritional information\n",
    "food_dataframe = food_dataframe[np.logical_not(food_dataframe['nutrition_grade_fr'].apply(lambda x: x is None)\n",
    "                                               # we do not check the values of nutrition-score-fr_100g or\n",
    "                                               # nutrition-score-uk_100g, because we know that they exists for exactly\n",
    "                                               # the same rows as nutrition_grade_fr\n",
    "                                               & food_dataframe['energy_100g'].apply(lambda x: x is None) \n",
    "                                               & food_dataframe['fat_100g'].apply(lambda x: x is None) \n",
    "                                               & food_dataframe['saturated-fat_100g'].apply(lambda x: x is None) \n",
    "                                               & food_dataframe['carbohydrates_100g'].apply(lambda x: x is None) \n",
    "                                               & food_dataframe['sugars_100g'].apply(lambda x: x is None) \n",
    "                                               & food_dataframe['fiber_100g'].apply(lambda x: x is None) \n",
    "                                               & food_dataframe['proteins_100g'].apply(lambda x: x is None)\n",
    "                                              )\n",
    "                               ]\n",
    "\n",
    "# Drop the rows with no purchase place\n",
    "food_dataframe = food_dataframe[food_dataframe['purchase_places'].apply(lambda x: x is not None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_rows = len(food_dataframe)\n",
    "for col in list(food_dataframe.columns):\n",
    "    none_values = np.count_nonzero(food_dataframe[col].apply(lambda x: x is None))\n",
    "    not_none_percentage = (total_rows - none_values) / total_rows\n",
    "    print('Rows that are **not** None in {col}: {p:.3%}'.format(col=col, p=not_none_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have rows for which the city tag is none and the purchase place is very vague, like France or the US. We need to take care of those. We will do it by creating a matching between the Open Food Facts dataset and a dataset including all cities of France."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Cities*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The dataframe: \n",
    "# https://www.insee.fr/fr/statistiques/3126432\n",
    "revenue_df = pd.read_excel(\"../data/raw/base-cc-filosofi-2014.xls\", skiprows=[0,1,2,3])\n",
    "\n",
    "# Drop rows that are not data and reset index\n",
    "revenue_df.drop(0, inplace=True)\n",
    "revenue_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "# Drop useless columns\n",
    "revenue_df = revenue_df.drop(columns=['Nombre de ménages fiscaux','Code géographique',\n",
    "                                      'dont part des salaires, traitements ou chômage (%)',\n",
    "                                      \"dont part des revenus d'activités non salariées (%)\",\n",
    "                                      'dont part des prestations familiales (%)',\n",
    "                                      'dont part des minima sociaux (%)',\n",
    "                                      'dont part des prestations logement (%)',\n",
    "                                      '1er décile du niveau de vie (€)',\n",
    "                                      '9e décile du niveau de vie (€)',\n",
    "                                      'Rapport inter-décile 9e décile/1er decile',\n",
    "                                      'Part des ménages fiscaux imposés (%)'\n",
    "                                     ]\n",
    "                            )\n",
    "\n",
    "# Translate the remaining columns to English\n",
    "revenue_df = revenue_df.rename(columns={\"ANNEE\" : \"Year\",\n",
    "                                        'Nombre de personnes dans les ménages fiscaux': \"Household inhabitants\", \n",
    "                                        'Médiane du niveau vie (€)': \"Median revenue euros\", \n",
    "                                        'Taux de pauvreté-Ensemble (%)' : \"Total poverty rate (%)\", \n",
    "                                        'Taux de pauvreté-moins de 30 ans (%)' : \"Poverty rate (-30) (%)\",\n",
    "                                        'Taux de pauvreté-30 à 39 ans  (%)' : \"Poverty rate (30-39) (%)\",\n",
    "                                        'Taux de pauvreté-40 à 49 ans (%)': \"Poverty rate (40-49) (%)\", \n",
    "                                        \"Taux de pauvreté-50 à 59 ans (%)\" :  \"Poverty rate (50-59) (%)\", \n",
    "                                        \"Taux de pauvreté-60 à 74 ans (%)\" :  \"Poverty rate (60-74) (%)\", \n",
    "                                        \"Taux de pauvreté-75 ans ou plus (%)\":  \"Poverty rate (75+) (%)\", \n",
    "                                        \"Taux de pauvreté-propriétaires (%)\" :  \"Poverty rate (house owners) (%)\", \n",
    "                                        \"Taux de pauvreté-locataires (%)\" :  \"Poverty rate (tenants) (%)\",\n",
    "                                        \"Part des revenus d'activité (%)\" :  \"Share of activity revenue (%)\", \n",
    "                                        'Part des pensions, retraites et rentes (%)' :  \"Share of retreat pension revenue (%)\", \n",
    "                                        'Part des revenus du patrimoine et autres revenus (%)' :  \"Share of heritage revenue and other (%)\",  \n",
    "                                        \"Part de l'ensemble des prestations sociales (%)\" :  \"Share of social benefits revenue (%)\", \n",
    "                                        'Part des impôts (%)' :  \"Share of taxes (%)\",\n",
    "                                        'Libellé géographique' : \"City name\"\n",
    "                                       }\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Total number of rows:\", len(revenue_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Total number of cities: \", len(list(set(revenue_df[\"City name\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Mapping cities - food products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will try to find a mapping between a city as present in the OpenFoodFacts dataset, and a city as know by the INSEE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to remove the accents\n",
    "import unicodedata as ud\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nkfd_form = ud.normalize('NFKD', str(input_str))\n",
    "    return u\"\".join([c for c in nkfd_form if not ud.combining(c)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us remove the accents, lowercase everything, and replace apostrophes by carets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_df['City name'] = revenue_df['City name'].apply(lambda x: remove_accents(x.lower().replace(\"'\", \"-\"))) \n",
    "revenue_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags for the cities (information present on the food items) are a list of comma separated tags, where each tag has the form: `city-name-department-name-france`, where `city-name` and `department-name` do not have accents and had their apostrophes and white spaces replaces by carets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tags of the cities per food item:\n",
    "# until now, the tags were a comma separated list of tags\n",
    "# we first split them\n",
    "cities_for_food = (food_dataframe['cities_tags'].str.split(',', expand=True)\n",
    "                    # then we create one entry per couple (food item, city tag)\n",
    "                    .stack()\n",
    "                    # we remove the index, as we will need to keep the column containing the id of the food item\n",
    "                    .reset_index()\n",
    "                    # and we drop duplicates and unnecessary columns\n",
    "                    .drop(columns='level_1').drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We move back to spark, because pandas does not offer the functionalities we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_city_names = spark.createDataFrame(revenue_df['City name'].drop_duplicates().reset_index())\n",
    "sdf_cities_for_food = spark.createDataFrame(cities_for_food)\n",
    "sdf_cities_for_food.createTempView('cities_for_food')\n",
    "sdf_city_names.createTempView('city_names')\n",
    "\n",
    "# we join our two dataframes, explanation below\n",
    "sdf_joined = spark.sql(\"\"\"\n",
    "SELECT city_names.index AS city_index, city_names.`City name` AS city_name, \n",
    "       aux.food_item_index, aux.city_tag_from_food_item\n",
    "FROM city_names\n",
    "JOIN (\n",
    "    SELECT MAX(LENGTH(city_names.`City name`)) AS length_city_name, \n",
    "           cities_for_food.level_0 AS food_item_index, cities_for_food.`0` AS city_tag_from_food_item\n",
    "    FROM cities_for_food\n",
    "    JOIN city_names\n",
    "    ON cities_for_food.`0` LIKE CONCAT(city_names.`City name`, '%')\n",
    "    GROUP BY cities_for_food.level_0, cities_for_food.`0`\n",
    ") AS aux\n",
    "ON aux.city_tag_from_food_item LIKE CONCAT(city_names.`City name`, '%')\n",
    "WHERE aux.length_city_name == LENGTH(city_names.`City name`)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A city's tag seems to usually be 'city-s-name-department-name-country-name'.\n",
    "A city's name is now 'city-s-name'.\n",
    "\n",
    "Thus we can join on the condition: `city_s_tag LIKE city_s_name + '%'`.\n",
    "\n",
    "Unfortunately, the city's name \"Saint Alban\", formatted as \"saint-alban\", will also match the tag \"saint-alban-les-eaux\", though these two cities maybe totally different and far away from one another.\n",
    "\n",
    "Thus, we want to kep only the longest city's name matching the tag, hence:\n",
    "```sql\n",
    "SELECT MAX(LENGTH(city_names.`City name`)) AS length_city_name, \n",
    "           cities_for_food.level_0 AS food_item_index, cities_for_food.`0` AS city_tag_from_food_item\n",
    "    FROM cities_for_food\n",
    "    JOIN city_names\n",
    "    ON cities_for_food.`0` LIKE CONCAT(city_names.`City name`, '%')\n",
    "    GROUP BY cities_for_food.level_0, cities_for_food.`0`\n",
    "```\n",
    "returning the size of the longest matching city name.\n",
    "\n",
    "We then have to re-do the same join and filter using the computed condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We write our dataframe to parquet, to be able to reuse it without needing to recompute everything\n",
    "sdf_joined.write.mode('overwrite').parquet(\"../data/interim/sdf_joined_city_names.parquet\")\n",
    "# Now we still have to join our food items and our cities, using the mapping we managed to get above\n",
    "# We switch back to pandas\n",
    "pdf_joined = sdf_joined.toPandas()\n",
    "pdf_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We join our mapping with the food dataframe\n",
    "food_df_for_join = pdf_joined.join(food_dataframe, on=\"food_item_index\")\n",
    "food_df_for_join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We join again on the income dataframe\n",
    "global_df = food_df_for_join.join(revenue_df, on=\"city_index\")\n",
    "# we can now drop the intermediary columns: city_name, and cities_tags\n",
    "global_df.drop(columns=['cities_tags'], inplace=True)\n",
    "global_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nutrition grade being a string, we cannot use it as is. Thus we turn it into a numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_to_number(x):\n",
    "    \"\"\"Convert the nutrition_grade to a numeric representation\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    x: string\n",
    "        Nutrition grade to convert.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numeric_representation: int\n",
    "        Numeric representation of the nutrition grade.\n",
    "    \"\"\"\n",
    "    if x == \"a\":\n",
    "        return 1\n",
    "    if x == \"b\":\n",
    "        return 2\n",
    "    if x == \"c\":\n",
    "        return 3\n",
    "    if x == \"d\":\n",
    "        return 4\n",
    "    if x == \"e\":\n",
    "        return 5\n",
    "    return 0\n",
    "global_df['nutrition_grade_numeric'] = global_df['nutrition_grade_fr'].apply(grade_to_number).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df.to_csv(\"../data/interim/clean_food_and_cities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER:** Several French towns have the exact same name. Thus, we have a few errors in our mapping. As this situation concerns less than 1% of the towns, it shouldn't impact our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Arrondissements\n",
    "\n",
    "In order to visualize data, we need to make the correspondences between the cities that we have and their respective arrondissement. The arrondissement is the territorial subdivision of the French territory that comes after the Department. There are around 340 arrondissements in France. We thought this subdivision was small enough to show the differences across the French territory but large enough for it to be visible on a map. In the same step, we kept the information relative to the other administrative areas in France, namely the departments and regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df = pd.read_csv('../data/interim/clean_food_and_cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new dataframe:\n",
    "# https://www.insee.fr/fr/information/2028028\n",
    "# Table d'appartenance géographique des communes au 1ᵉʳ janvier 2017\n",
    "city_arr = pd.read_excel('../data/raw/table-appartenance-geo-communes-17.xls', skiprows=[0, 1, 2, 3])\n",
    "city_arr.drop(0, inplace=True)\n",
    "city_arr.reset_index(inplace=True, drop=True)\n",
    "\n",
    "city_arr[\"city_name\"] = city_arr['Libellé géographique'].apply(\n",
    "    # Remove accents, lowercase, and replace spaces ad apostrophes by '-'\n",
    "    lambda x: remove_accents(x.lower().replace(\"'\", '-').replace(\" \", \"-\"))\n",
    ") \n",
    "\n",
    "# Drop useless columns\n",
    "city_arr = city_arr.drop(columns=['Intercommunalité - Métropole', \"Nature d'EPCI\", \"Zone d'emploi 2010\", \n",
    "                       \"Unité urbaine 2010\", \"Tranche d'unité urbaine 2014\", \n",
    "                       \"Tranche détaillée d'unité urbaine 2014\", \"Aire urbaine 2010\", \n",
    "                       \"Tranche d'aire urbaine 2014\", \"Bassin de vie 2012\"]\n",
    "             )\n",
    "city_arr.rename(columns={'Libellé géographique' : 'City name'}, inplace=True)\n",
    "city_arr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a mapping to available geojson files for the administrative areas of France, we have to format our informations about the belonging of a city to an administrative area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To match the geojson file we found, we have to format the code of our arrondissements\n",
    "city_arr[\"custom_arrondissement_code\"] = city_arr[\"Département\"] + \"00\" + city_arr['Arrondissement'].astype('str').apply(lambda x: x[-1])\n",
    "city_arr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_arr.to_csv(\"../data/interim/city_region_arrondissement.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_arrond = global_df.merge(city_arr, left_on=\"city_name\", right_on=\"city_name\", how='inner')\n",
    "global_arrond.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating to English the columns and keeping only the relevant columns gives us this dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_arrond = global_arrond.rename(columns={\"Département\" : \"Department\", \"Région\" : \"Region\", \"City name_y\": \"City name\"})\n",
    "global_arrond = global_arrond[['food_item_index', 'nutrition-score-fr_100g', 'nutrition-score-uk_100g',\n",
    "                       'nutrition_grade_numeric', 'serving_size', 'energy_100g', 'fat_100g', \n",
    "                       'saturated-fat_100g', 'proteins_100g', 'carbohydrates_100g', 'sugars_100g', \n",
    "                       'fiber_100g', 'Median revenue euros', 'Total poverty rate (%)', 'Poverty rate (-30) (%)',\n",
    "                       'Poverty rate (30-39) (%)', 'Poverty rate (40-49) (%)',\n",
    "                       'Poverty rate (50-59) (%)', 'Poverty rate (60-74) (%)',\n",
    "                       'Poverty rate (75+) (%)', 'Poverty rate (house owners) (%)',\n",
    "                       'Poverty rate (tenants) (%)', 'Share of activity revenue (%)',\n",
    "                       'Share of retreat pension revenue (%)',\n",
    "                       'Share of heritage revenue and other (%)',\n",
    "                       'Share of social benefits revenue (%)', 'Share of taxes (%)', 'City name', \n",
    "                       'Department', 'Region', 'custom_arrondissement_code', 'city_name', 'city_tag_from_food_item']]\n",
    "global_arrond.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_arrond.to_csv(\"../data/interim/clean_food_cities_arrond.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of our data\n",
    "\n",
    "Now that the data cleaning is done, and in order to analyse the data, we need to proceed with the next steps: \n",
    "\n",
    "2. Exploration of the distribution of the nutrition grades at each geographical level for the richest and poorest zone\n",
    "\n",
    "3. Distribution of the products frequency according to their nutrition grade\n",
    "\n",
    "4. Implementation of two additional custom nutrition scores\n",
    "\n",
    "5. Dimensionality reduction\n",
    "\n",
    "6. Data aggregation according to a certain geographical level (city, arrondissement, department or region)\n",
    "\n",
    "7. Attempt at clustering\n",
    "\n",
    "8. Correlation exploration\n",
    "\n",
    "9. Geographic visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('../data/processed/clean_food_cities_arrond.csv', low_memory=False)\n",
    "dataframe = dataframe.drop(columns=['Unnamed: 0'])\n",
    "dataframe.rename(columns={\"City name\": \"City\", \"custom_arrondissement_code\": \"Arrondissement\"}, inplace=True)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Distribution of the nutrition grades at each geographical level for the richest and poorest zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_for_area(df, area_column, area_name):\n",
    "    \"\"\"Display the distribution of the numeric nutrition grade for the richest and poorest area\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        The dataframe containing the needed information.\n",
    "    area_column: string\n",
    "        Name of the column containing the information about the area.\n",
    "    area_name: string\n",
    "        Name of the area.\n",
    "    \"\"\"\n",
    "    revenue = df.groupby(area_column).mean().sort_values(by=['Median revenue euros'], ascending=False)\n",
    "    rich = revenue.iloc[[0]]\n",
    "    rich.reset_index(inplace=True)\n",
    "    poor = revenue.iloc[[-1]]\n",
    "    poor.reset_index(inplace=True)\n",
    "    \n",
    "    departments = [list(rich[area_column].values)[0], list(poor[area_column].values)[0]]\n",
    "    list_dummy_grades = ['nutrition_grade_numeric_0', 'nutrition_grade_numeric_1',\n",
    "                     'nutrition_grade_numeric_2', 'nutrition_grade_numeric_3',\n",
    "                     'nutrition_grade_numeric_4', 'nutrition_grade_numeric_5']\n",
    "    \n",
    "    dummies = pd.get_dummies(df, columns=['nutrition_grade_numeric']).groupby(area_column).sum()\n",
    "    dummies.reset_index(inplace=True)\n",
    "    dummies = dummies[dummies[area_column].apply(lambda x: x in departments)][list_dummy_grades]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "    fig.suptitle(\"Distribution of the nutrition grades' frequencies for the richest and the poorest {}\".format(area_name),\n",
    "                 y=1.08, fontweight=\"bold\")\n",
    "\n",
    "    sns.barplot(x=[0, 1, 2, 3, 4, 5], y=dummies.iloc[0].values, ax=ax1)\n",
    "    sns.barplot(x=[0, 1, 2, 3, 4, 5], y=dummies.iloc[1].values, ax=ax2)\n",
    "    ax1.set_xlabel('Nutrition grade')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax2.set_xlabel('Nutrition grade')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax1.set_title(\"Richest {}\".format(area_name))\n",
    "    ax2.set_title(\"Poorest {}\".format(area_name))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_for_area(dataframe, 'City', 'city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_for_area(dataframe, 'Arrondissement', 'arrondissement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_for_area(dataframe, 'Department', 'department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_for_area(dataframe, 'Region', 'region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that rich and poor areas can have very different distribution of the healthiness of the available products.\n",
    "However, we cannot infer a correlation from only two areas.\n",
    "\n",
    "Also, we see that the number of products varies a lot between areas. This was expected, as the products are entered by the users of OpenFoodFacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implementation of custom nutrition scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce two more nutrition scores: calory density (the calories per serving) and calory deviation.\n",
    "\n",
    "The *calory deviation* is computed according to the nutritional standards that among the calories we consume in a day, $21\\%$ should come from proteins, $53\\%$ from carbohydrates and $26\\%$ from fat. We then compute a deviation from this standard for each product. \n",
    "\n",
    "* In the first case (calory density), the metric describes how largely a product is packed with calories. Since obesity is often linked with the density in calories of the ingested food items, this metric might be useful.\n",
    "\n",
    "* In the second case (calory deviation), we aim at observing whether the products follow a healthy calory distribution. This metric would show us if a certain region has bad nutrition habits with respect to the ratios of nutriments they should eat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We consider only the energy per 100g and the serving size\n",
    "cals = dataframe[['energy_100g', 'serving_size']]\n",
    "# We drop nan values, and serving sizes that are not relevant\n",
    "cals = cals.dropna()\n",
    "cals = cals.drop(cals[cals['serving_size'].str.contains(\"mg\")].index)\n",
    "cals = cals.drop(cals[cals['serving_size'].str.contains('oeuf')].index)\n",
    "cals = cals.drop(cals[cals['serving_size'].str.contains('Deux tranches g')].index)\n",
    "# We keep only the number of units of the usual measure unit for the product\n",
    "cal = cals['serving_size'].str.replace('.*?(?P<number>[0-9.,]+)\\s*(G|g|f|ml|mL).*', '\\\\g<number>')\n",
    "cal = cal.str.replace('.*?(?P<number>[0-9.,]+)\\s*(cl|cL).*', '\\\\g<number>0')\n",
    "cal = cal.str.replace('.*?(?P<number>[0-9.,]+)\\s*(Kg|L|l).*', '\\\\g<number>000')\n",
    "# We remove values that are composed only of letters\n",
    "cal = cal.str.replace('[^0-9]+', '')\n",
    "cal = cal.replace('', np.nan)\n",
    "cal = cal.dropna()\n",
    "# We cast everything to float\n",
    "cal = cal.str.replace(\",\", \".\").astype('float')\n",
    "cals.serving_size = cal\n",
    "# and reinsert it in our dataframe\n",
    "dataframe.loc[:, 'serving_size'] = cal\n",
    "\n",
    "cal_density = cals.serving_size * cals.energy_100g / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "Proteins bring 4 calories per gram, and should constitue 21% of our calories.\n",
    "\n",
    "Carbohydrates also bring 4 calories per gram, and should constitue 53% of our calories.\n",
    "\n",
    "Fat brings 9 calories per gram, and should constitue the remaining 26% of our calories.\n",
    "\n",
    "Hence the computations below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy has to be divided by 4.184 to convert from kiloJoules to kilocalories\n",
    "energy_in_kcal_100g = dataframe.energy_100g / 4.184\n",
    "cal_deviation = ((dataframe.fat_100g * 9 / energy_in_kcal_100g - 0.26)**2 + \n",
    "                (dataframe.carbohydrates_100g * 4 / energy_in_kcal_100g - 0.53)**2 +\n",
    "                (dataframe.proteins_100g * 4 / energy_in_kcal_100g - 0.21)**2)/3\n",
    "# We remove unknown and infinite values\n",
    "cal_deviation.replace(np.infty, np.nan, inplace=True)\n",
    "cal_deviation.dropna(inplace=True)\n",
    "cal_deviation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We insert our columns in our dataframe\n",
    "dataframe.loc[:, 'cal_density'] = cal_density\n",
    "dataframe.loc[:, 'cal_deviation'] = cal_deviation\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Distribution of the products per nutrition grade and per median revenue of the city where it is sold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nutrition grade has integer values between 1 and 5, 1 indicating healthy food items and 5 describing unhealthy items.\n",
    "\n",
    "* Nutrition score (UK or Fr.) has values between -15 and 40. -15 indicates a healthy food item, 40 indicates an unhealthy food item. \n",
    "\n",
    "The correspondence between the two of them for solid foods is:\n",
    "\n",
    "* Nutrition grade = 1 : nutrition score $\\in [-15, : -1]$\n",
    "* Nutrition grade = 2 : nutrition score $\\in [0, 2]$\n",
    "* Nutrition grade = 3 : nutrition score $\\in [3, 10]$\n",
    "* Nutrition grade = 4 : nutrition score $\\in [11, 18]$\n",
    "* Nutrition grade = 5 : nutrition score $\\in [19, 40]$\n",
    "\n",
    "The correspondence between the two of them for beverages is:\n",
    "\n",
    "* Nutrition grade = 1 : Water\n",
    "* Nutrition grade = 2 : nutrition score $\\in [-15, 1]$\n",
    "* Nutrition grade = 3 : nutrition score $\\in [2, 5]$\n",
    "* Nutrition grade = 4 : nutrition score $\\in [6, 9]$\n",
    "* Nutrition grade = 5 : nutrition score $\\in [10, 40]$\n",
    "\n",
    "https://fr.openfoodfacts.org/nutriscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 4))\n",
    "f.suptitle(\"Nutrition grades and scores for the overall of food item sold in France\", \n",
    "           y=1.08, fontweight=\"bold\")\n",
    "sns.distplot(dataframe['nutrition_grade_numeric'].dropna().values, ax=ax1, norm_hist=True)\n",
    "ax1.set_title(\"Histogram of the French nutrition grade\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_xlabel(\"Nutrition grade (healthiest to unhealthiest)\")\n",
    "\n",
    "sns.distplot(dataframe['nutrition-score-fr_100g'].dropna().values, ax=ax2)\n",
    "ax2.set_title(\"Histogram of the French nutrition score\")\n",
    "ax2.set_ylabel(\"Frequency\")\n",
    "ax2.set_xlabel(\"Nutrition score (healthiest to unhealthiest)\")\n",
    "\n",
    "sns.distplot(dataframe['nutrition-score-uk_100g'].dropna().values, ax=ax3)\n",
    "ax3.set_title(\"Histogram of the UK nutrition score\")\n",
    "ax3.set_ylabel(\"Frequency\")\n",
    "ax3.set_xlabel(\"Nutrition score (healthiest to unhealthiest)\")\n",
    "\n",
    "f.savefig(\"../docs/img/portfolio/nutrigrade_hist.pdf\")\n",
    "plt.show()\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.distplot(dataframe['cal_density'].dropna().values, ax=ax1, norm_hist=True)\n",
    "ax1.set_title(\"Histogram of the calory density\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_xlabel(\"Calory density (kcal per serving size)\")\n",
    "\n",
    "sns.distplot(dataframe['cal_deviation'].dropna().values, ax=ax2)\n",
    "ax2.set_title(\"Histogram of the calory deviation\")\n",
    "ax2.set_ylabel(\"Frequency\")\n",
    "ax2.set_xlabel(\"Calory deviation (from recommended nutrition standards)\")\n",
    "\n",
    "f.savefig(\"../docs/img/portfolio/nutrigrade_hist2.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reminder: `0` is equivalent to no data for Fr. nutrition grade.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the nutrition grade histogram, we see that most of the products are labeled with a `3` or higher, which indicates that healthy products are rare in comparison. \n",
    "\n",
    "When looking at the nutrition score histograms, we can see two peaks. One where the products are around 0 (i.e. nutrition grade of 2) and the other one with products around 15 (nutrition grade of 4). Most products are however well above 0 (i.e. nutrition grade of 2 or higher), hence the lack of healthy products is reinforced. \n",
    "\n",
    "Let's now look at the distribution of the median revenue of the cities where the products are sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10, 7))\n",
    "sns.distplot(dataframe['Median revenue euros'].dropna().values, norm_hist=False)\n",
    "plt.xlabel('Median revenue (€)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the median revenue of \\n the cities where food items are sold')\n",
    "f.savefig(\"../docs/img/portfolio/revenue_hist.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of our products are sold in cities where the median revenue is around 20 000 €. This histogram is a tight gaussian that shows that we do not have many values for cities where the median revenue is lower than 15 000 € a year, nor for cities having a median yearly revenue of more than 25 000 €. This corresponds in France to the **lower middle class.**\n",
    "\n",
    "We can already see that this is going to be a problem when we analyse our data. Indeed, having most of our products belonging to a moderate revenue biases our anaylisis and will make us more likely to remove outliers that are in extreme ranges of revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Dimensionality reduction\n",
    "\n",
    "The aim of this section is to use Mutual Information in order to decide which columns in our dataframe are worth keeping. Indeed, we have numerous poverty markers and five different nutrition scores. We wish to see if among these categories, some columns overlap so much that we can exclude them from our analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some non-numeric columns in our dataframe that we cannot use as is for our dimensionality reduction. Luckily, these columns are all the geographical ones (region, city, arrondissement and department). We do not wish to eliminate either of those columns because they are useful for aggregation and visualization. \n",
    "\n",
    "Hence, the numeric columns selected below are the numerical attributes that we are going to compare to each other using Mutual Information and they describe the nutritional features of the products and the economic characteristics of the cities where they are sold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Checking which columns have the most information\n",
    "\n",
    "Once again, we try to find the columns for which we are lacking information: we find that it is wise to avoid using them, even before we try to compute the Mutual Information among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['food_item_index', 'nutrition-score-fr_100g', \n",
    "                'nutrition-score-uk_100g','nutrition_grade_numeric', 'serving_size', 'energy_100g', \n",
    "                'fat_100g','saturated-fat_100g', 'proteins_100g', 'carbohydrates_100g','sugars_100g', \n",
    "                'fiber_100g', 'Median revenue euros','Total poverty rate (%)', 'Poverty rate (-30) (%)',\n",
    "               'Poverty rate (30-39) (%)', 'Poverty rate (40-49) (%)',\n",
    "               'Poverty rate (50-59) (%)', 'Poverty rate (60-74) (%)',\n",
    "               'Poverty rate (75+) (%)', 'Poverty rate (house owners) (%)',\n",
    "               'Poverty rate (tenants) (%)', 'Share of activity revenue (%)',\n",
    "               'Share of retreat pension revenue (%)',\n",
    "               'Share of heritage revenue and other (%)',\n",
    "               'Share of social benefits revenue (%)', 'Share of taxes (%)',\n",
    "               'cal_density', 'cal_deviation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in numeric_cols:\n",
    "    nonna = len(dataframe[dataframe[column].apply(lambda x: not np.isnan(x))]) / len(dataframe)\n",
    "    print('{:<45s}{:<30s}{:<25.1%}'.format(column, 'Number of non-na:', nonna))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can observe that there are some columns that have very few non-NaN values. In the nutritional group, all columns have more than $50\\%$ of actual values. However, in the economical group, we can see that most columns are well below $50 \\%$: all the poverty rates and all the shares of revenue. In fact, for some columns we only have around $10 \\%$ of the values! \n",
    "\n",
    "At this stage of the analysis, we decided to keep all nutritional columns. Indeed, all of them are probably useful for our analysis, however if we keep economical columns that have a lot of NaN values within them, we will lose information when we use `dropna()`. \n",
    "\n",
    "We also decided to keep all economic columns that have a reasonable number of values. We will then see if we can discard any. The economic columns that we keep are the ones having at least $40 \\%$ of the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.drop(columns=['Poverty rate (-30) (%)', 'Poverty rate (30-39) (%)', \n",
    "                                    'Poverty rate (40-49) (%)', 'Poverty rate (50-59) (%)', \n",
    "                                    'Poverty rate (60-74) (%)', 'Poverty rate (75+) (%)', \n",
    "                                    'Poverty rate (house owners) (%)','Poverty rate (tenants) (%)', \n",
    "                                    'Share of taxes (%)'\n",
    "                                   ]\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Mutual information on economical factors\n",
    "\n",
    "We make the hypothesis that economical factors are linked with one another, and investigate this hypothesis in the following. We do so by using Mutual Information (MI). \n",
    "\n",
    "This measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency. We thought also about using PCA but as we are looking into selecting features from our original data, MI gives an easier interpretation of the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_df = dataframe.copy()\n",
    "\n",
    "eco_df = eco_df.drop(columns=['food_item_index', 'nutrition-score-fr_100g', \n",
    "                              'nutrition_grade_numeric', 'energy_100g', 'fat_100g', \n",
    "                              'saturated-fat_100g', 'proteins_100g', 'fiber_100g', \n",
    "                              'carbohydrates_100g', 'sugars_100g', 'nutrition-score-uk_100g', \n",
    "                              'serving_size', 'City', 'Department', 'Region', 'Arrondissement',\n",
    "                              'cal_density', 'cal_deviation'\n",
    "                            ]).dropna()\n",
    "eco_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We investigate the Mutual Information of each pair of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the mutual information for each pair of columns related to the economics of a city\n",
    "mutual_info = pd.DataFrame(0, index=eco_df.columns, columns=eco_df.columns)\n",
    "\n",
    "for (i, feat) in enumerate(eco_df.columns):\n",
    "    mutual_info.loc[feat, eco_df.columns[i:]] = mutual_info_regression(eco_df[eco_df.columns[i:]], eco_df[feat])\n",
    "    mutual_info.loc[feat, eco_df.columns[i:]] = mutual_info.loc[feat, eco_df.columns[i:]]/mutual_info.loc[feat,feat]\n",
    "    mutual_info.loc[eco_df.columns[i:], feat] = mutual_info.loc[feat, eco_df.columns[i:]]\n",
    "    \n",
    "plot_cols = ['Median revenue (€)',\n",
    "       'Total poverty rate (%)', 'Share of \\n activity revenue (%)',\n",
    "       'Share of retreat \\n pension revenue (%)',\n",
    "       'Share of heritage \\n revenue and other (%)',\n",
    "       'Share of social \\n benefits revenue (%)']\n",
    "\n",
    "# And we display it as a mutual information matrix\n",
    "matrix  = np.array(mutual_info)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "im = ax.imshow(matrix, vmin=0, vmax=1, cmap='Reds')\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(plot_cols)))\n",
    "ax.set_yticks(np.arange(len(plot_cols)))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(plot_cols)\n",
    "ax.set_yticklabels(plot_cols)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(plot_cols)):\n",
    "    for j in range(len(plot_cols)):\n",
    "        string = \"{0:.2f}\".format(matrix[i, j])\n",
    "        text = ax.text(j, i, string,\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title('Mutual information matrix on economic markers', fontweight='bold', y=1.03)\n",
    "fig.colorbar(im)\n",
    "plt.savefig('../docs/img/portfolio/eco_mutualinfo.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix shows the Mutual Information that each column has with the others, normalized. Mutual Information is $1$ when the compared features are the same (i.e. when comparing a column with itself) and $0$ when the two columns do not have any common patterns.\n",
    "\n",
    "As we can see, all columns are extremely related with each other. We do not have Mutual Information coefficients falling below $0.6$, and for most of the column pairs the MI coefficient is well above $0.7$. \n",
    "\n",
    "Moreover, we can also see that the ***Median revenue and Total Poverty Rate*** are the columns having the highest MI coefficients with all of the other columns. This suggests us that if we were to choose features to keep, we would be safer to keep those as they explain better all of the other features. \n",
    "\n",
    "However, we saw that among all these economic features, the only one having an acceptable percentage of values is Median Revenue. We decided to keep only that one in order not to loose too many information on our dataset. Since Median Revenue has high MI coefficients with all the other columns, it is safe to keep only that one because it quite embeds the information of the other economic markers.  \n",
    "\n",
    "To sum up, and for now, this leaves us with:\n",
    "\n",
    "* 1 column describing the food item's identifier:\n",
    "    - food item index\n",
    "\n",
    "* 11 columns describing nutritional features:\n",
    "    - nutrition-score-fr_100g            \n",
    "    - nutrition-score-uk_100g            \n",
    "    - nutrition_grade_numeric           \n",
    "    - serving_size           \n",
    "    - energy_100g                        \n",
    "    - fat_100g                           \n",
    "    - saturated-fat_100g                 \n",
    "    - proteins_100g                      \n",
    "    - carbohydrates_100g                 \n",
    "    - sugars_100g                        \n",
    "    - fiber_100g\n",
    "\n",
    "* 1 column describing economic features: \n",
    "    - Median revenue euros\n",
    "    \n",
    "* 4 columns describing geographical features\n",
    "    - City\n",
    "    - Arrondissement\n",
    "    - Department\n",
    "    - Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep only the wanted columns in our dataframe\n",
    "dataframe = dataframe[['food_item_index', 'nutrition-score-fr_100g', 'nutrition-score-uk_100g',\n",
    "                       'nutrition_grade_numeric', 'serving_size', 'energy_100g', 'fat_100g',\n",
    "                       'saturated-fat_100g', 'proteins_100g', 'carbohydrates_100g',\n",
    "                       'sugars_100g', 'fiber_100g', 'cal_density', 'cal_deviation', \n",
    "                       'Median revenue euros', 'City', 'Department', 'Region', 'Arrondissement']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Mutual Information on the rest of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the important elements from the two datasets we can apply feature reduction on the nutritional columns using again Mutual Information (MI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = dataframe.drop(columns=['food_item_index', 'Department', 'Arrondissement', 'City', 'Region']).dropna()\n",
    "\n",
    "mutual_info = pd.DataFrame(0, index=new_df.columns, columns=new_df.columns)\n",
    "\n",
    "for (i, feat) in enumerate(new_df.columns):\n",
    "    mutual_info.loc[feat, new_df.columns[i:]] = mutual_info_regression(new_df[new_df.columns[i:]], new_df[feat])\n",
    "    mutual_info.loc[feat, new_df.columns[i:]] = mutual_info.loc[feat, new_df.columns[i:]]/mutual_info.loc[feat,feat]\n",
    "    mutual_info.loc[new_df.columns[i:], feat] = mutual_info.loc[feat, new_df.columns[i:]]\n",
    "\n",
    "mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_cols = list(mutual_info.columns)\n",
    "\n",
    "# And we display it as a mutual information matrix\n",
    "matrix  = np.array(mutual_info)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "im = ax.imshow(matrix, vmin=0, vmax=1, cmap='Reds')\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(plot_cols)))\n",
    "ax.set_yticks(np.arange(len(plot_cols)))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(plot_cols)\n",
    "ax.set_yticklabels(plot_cols)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(plot_cols)):\n",
    "    for j in range(len(plot_cols)):\n",
    "        string = \"{0:.2f}\".format(matrix[i, j])\n",
    "        text = ax.text(j, i, string,\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title('Mutual information matrix on all columns', fontweight='bold', y=1.03)\n",
    "fig.colorbar(im)\n",
    "plt.savefig('../docs/img/portfolio/mutualinfotot.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the figure above, we deduce that the MI between nutrition-score-fr_100g and nutrition-score-uk_100g is very high ($0.92$). However, none of the other columns seem to have a high enough MI coefficient so as to be remved from the dataframe (they are all around or below $0.5$). This leads us to the drop of **nutrition-score-uk_100g** only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.drop(columns=['nutrition-score-uk_100g'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to remove the DOM-TOMS since the cultural differences are too large (Guadeloupe, Guyane, etc). We have observed that given our current dataframe, we only have one product being sold in the whole region of Corsica. Since this could really affect our analysis, given that we aim at aggregating by region, we remove Corsica from the dataset.\n",
    "\n",
    "Then, we transform the dataframe to be able to do the analysis at four different increasing levels, City, Arrondissement, Departement and Region level. For this reason, we create four different aggregations of the dataframe using each time the corresponding column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOM-TOM Regions and Corsica\n",
    "dom_tom_corse = [1, 2, 4, 94]\n",
    "# DOM-TOM department\n",
    "dom_tom_dep = '97'\n",
    "dataframe = dataframe[dataframe['Region'].apply(lambda x: x not in dom_tom_corse)]\n",
    "dataframe = dataframe[dataframe['Department'].apply(lambda x: x  != dom_tom_dep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the cleanest dataframe that we have, hence our final dataframe. We need to save it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv(\"../data/processed/final_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reading it, we have to convert the geographic regions to string again and add '0' at the beginning of some geographic locations in order to make the correspondence with the geojson files later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food_item_index</th>\n",
       "      <th>nutrition-score-fr_100g</th>\n",
       "      <th>nutrition_grade_numeric</th>\n",
       "      <th>serving_size</th>\n",
       "      <th>energy_100g</th>\n",
       "      <th>fat_100g</th>\n",
       "      <th>saturated-fat_100g</th>\n",
       "      <th>proteins_100g</th>\n",
       "      <th>carbohydrates_100g</th>\n",
       "      <th>sugars_100g</th>\n",
       "      <th>fiber_100g</th>\n",
       "      <th>cal_density</th>\n",
       "      <th>cal_deviation</th>\n",
       "      <th>Median revenue euros</th>\n",
       "      <th>City</th>\n",
       "      <th>Department</th>\n",
       "      <th>Region</th>\n",
       "      <th>Arrondissement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>431441</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>883.0</td>\n",
       "      <td>12.00</td>\n",
       "      <td>4.40</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.133987</td>\n",
       "      <td>21885.714286</td>\n",
       "      <td>Apremont</td>\n",
       "      <td>01</td>\n",
       "      <td>84</td>\n",
       "      <td>01004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>348749</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>703.0</td>\n",
       "      <td>9.30</td>\n",
       "      <td>3.80</td>\n",
       "      <td>19.00</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.116812</td>\n",
       "      <td>21885.714286</td>\n",
       "      <td>Apremont</td>\n",
       "      <td>01</td>\n",
       "      <td>84</td>\n",
       "      <td>01004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>348720</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>900.0</td>\n",
       "      <td>12.00</td>\n",
       "      <td>4.40</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.130923</td>\n",
       "      <td>21885.714286</td>\n",
       "      <td>Apremont</td>\n",
       "      <td>01</td>\n",
       "      <td>84</td>\n",
       "      <td>01004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>348741</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>3.66</td>\n",
       "      <td>1.48</td>\n",
       "      <td>21.13</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257.0</td>\n",
       "      <td>0.157268</td>\n",
       "      <td>21885.714286</td>\n",
       "      <td>Apremont</td>\n",
       "      <td>01</td>\n",
       "      <td>84</td>\n",
       "      <td>01004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>348708</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>9.40</td>\n",
       "      <td>3.90</td>\n",
       "      <td>18.70</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>699.0</td>\n",
       "      <td>0.117324</td>\n",
       "      <td>21885.714286</td>\n",
       "      <td>Apremont</td>\n",
       "      <td>01</td>\n",
       "      <td>84</td>\n",
       "      <td>01004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   food_item_index  nutrition-score-fr_100g  nutrition_grade_numeric  \\\n",
       "0           431441                     16.0                        4   \n",
       "1           348749                     15.0                        4   \n",
       "2           348720                     16.0                        4   \n",
       "3           348741                     11.0                        4   \n",
       "4           348708                     15.0                        4   \n",
       "\n",
       "   serving_size  energy_100g  fat_100g  saturated-fat_100g  proteins_100g  \\\n",
       "0           NaN        883.0     12.00                4.40          25.00   \n",
       "1           NaN        703.0      9.30                3.80          19.00   \n",
       "2           NaN        900.0     12.00                4.40          25.00   \n",
       "3          50.0        514.0      3.66                1.48          21.13   \n",
       "4         100.0        699.0      9.40                3.90          18.70   \n",
       "\n",
       "   carbohydrates_100g  sugars_100g  fiber_100g  cal_density  cal_deviation  \\\n",
       "0                0.60         0.50         0.5          NaN       0.133987   \n",
       "1                1.90         1.20         0.3          NaN       0.116812   \n",
       "2                0.60         0.50         NaN          NaN       0.130923   \n",
       "3                1.13         1.13         NaN        257.0       0.157268   \n",
       "4                1.90         1.70         NaN        699.0       0.117324   \n",
       "\n",
       "   Median revenue euros      City Department Region Arrondissement  \n",
       "0          21885.714286  Apremont         01     84          01004  \n",
       "1          21885.714286  Apremont         01     84          01004  \n",
       "2          21885.714286  Apremont         01     84          01004  \n",
       "3          21885.714286  Apremont         01     84          01004  \n",
       "4          21885.714286  Apremont         01     84          01004  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"../data/processed/final_df.csv\", low_memory=False)\n",
    "dataframe = dataframe.drop(columns=['Unnamed: 0'])\n",
    "dataframe['Arrondissement'] = dataframe['Arrondissement'].astype(str)\n",
    "dataframe['Arrondissement'] = dataframe['Arrondissement'].apply(lambda x : '0' + x if len(x)<5 else x)\n",
    "dataframe['Region'] = dataframe['Region'].apply(lambda x: str(x))\n",
    "dataframe['Department'] = dataframe['Department'].apply(lambda x: str(x))\n",
    "dataframe['Department'] = dataframe['Department'].apply(lambda x : '0' + x if len(x) == 1 else x)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutritional_columns = ['nutrition-score-fr_100g', 'nutrition_grade_numeric',\n",
    "                       'energy_100g', 'fat_100g', 'saturated-fat_100g', 'proteins_100g', \n",
    "                       'carbohydrates_100g', 'sugars_100g', 'fiber_100g', 'cal_deviation', 'cal_density']\n",
    "economic_columns = ['Median revenue euros']\n",
    "geo_columns = ['City','Department', 'Region', 'Arrondissement']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we aggregate according to each geographic subdivision. To do so, we group our dataframe by said subdivision. Each row will be a geographic location and the columns are chosen to be the median of the features for each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(df, area_column):\n",
    "    \"\"\"Aggregate our nutritional data per geographic area\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Dataframe containing the relevant information.\n",
    "    area_column: string\n",
    "        Name of the column coding the geographic area.\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    median_df: pandas.Dataframe\n",
    "        Dataframe conatining the median value for each nutritional feature, grouped by geographic area.\n",
    "    \"\"\"\n",
    "    drop_cols = geo_columns.copy()\n",
    "    drop_cols.remove(area_column)\n",
    "    \n",
    "    median_df = df.drop(columns=drop_cols).dropna().groupby(area_column).median()\n",
    "    \n",
    "    return median_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_city = aggregate(dataframe, 'City') \n",
    "aggregated_arrondissement = aggregate(dataframe, 'Arrondissement') \n",
    "aggregated_department = aggregate(dataframe, 'Department') \n",
    "aggregated_region = aggregate(dataframe, 'Region') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a dataframe per geographic level, that aggregates by median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Cities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(df):\n",
    "    \"\"\"Display the scatter plots representing the relationship between each nutrition attribute \n",
    "    and the Median Revenue\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Dataframe containing the relevant information.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(3, 4, figsize=(25, 18))\n",
    "    fig.suptitle(\"Scatter plots of the relationships between each nutrition attribute and the Median Revenue\", \n",
    "                fontweight=\"bold\")\n",
    "    i = 0;\n",
    "    j = 0;\n",
    "    count = 1;\n",
    "    for col in nutritional_columns:\n",
    "        df.plot.scatter(x='Median revenue euros', y=col, ax=axs[i, j])\n",
    "        axs[i, j].set_title(col)\n",
    "        if count % 4 == 0:\n",
    "            i = i + 1;\n",
    "        j = (j + 1) % 4;\n",
    "        count = count + 1\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_scatter(aggregated_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we cannot see clear clusters, especially not across different median revenues. The datapoints are all concentrated around 20000 €, and we can see some vague clusters for the graph plotting the **carbohydrates** against **median revenue** (plot (2,4)). The clusters are however not differentiated according to median revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Arrondissements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_scatter(aggregated_arrondissement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, no clear clusters emerge from these scatter plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Departments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(aggregated_department)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, all blobs are concentrated around 20000€ and we cannot extract actual clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Regions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(aggregated_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we cannot find clusters but we might get a hint of some correlations. These links are explored below. \n",
    "\n",
    "In order to make sure that no good clusters can be found, we used DBSCAN with several parameters. We plotted only the results for **nutritional score** against **median revenue** since it was the most significant plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(df, eps=0.3, min_samples=10):\n",
    "    \"\"\"Compute and display clusters\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Dataframe containing the needed information.\n",
    "    eps: float\n",
    "        Distance to consider for neighbors for the DBSCAN algorithm.\n",
    "    min_samples: int\n",
    "        Minimum number of points to consider the set of points as a cluster and no outliers.\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    labels: numpy.ndarray\n",
    "        The labels associated with each point.\n",
    "    \"\"\"\n",
    "    X = StandardScaler().fit_transform(df)\n",
    "\n",
    "    # Compute DBSCAN\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "\n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    print('Estimated number of noise points: %d' % n_noise_)\n",
    "    if n_clusters_ > 0:\n",
    "        print(\"Silhouette Coefficient: %0.3f\"\n",
    "              % metrics.silhouette_score(X, labels))\n",
    "    else:\n",
    "        print(\"Silhouette Coefficient could not be computed for 0 clusters\")\n",
    "\n",
    "    # Plot result\n",
    "    # Black removed and is used for noise instead.\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each)\n",
    "              for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    plt.figure(figsize=(10,7))\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = [0, 0, 0, 1]\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "\n",
    "        xy = X[class_member_mask & core_samples_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "                 markeredgecolor='k', markersize=14)\n",
    "\n",
    "        xy = X[class_member_mask & ~core_samples_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "                 markeredgecolor='k', markersize=6)\n",
    "\n",
    "    plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "    plt.show()\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = cluster(aggregated_city[['Median revenue euros', 'nutrition-score-fr_100g']], 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = cluster(aggregated_arrondissement[['Median revenue euros', 'nutrition-score-fr_100g']], 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = cluster(aggregated_department[['Median revenue euros', 'nutrition-score-fr_100g']], 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = cluster(aggregated_region[['Median revenue euros', 'energy_100g']], 0.7, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted above, the maximum Silhouette coefficient is always found for one cluster. There are no meaningful distinct clusters.\n",
    "\n",
    "These are just examples. We tested the clustering in two dimensions for each nutritional feature, for each geographical location, without finding any meaningful clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral clustering\n",
    "\n",
    "In multivariate statistics and the clustering of data, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. \n",
    "\n",
    "This technique relies in the representation of data as a graph. A graph is composed of nodes linked together by edges. The nodes usually represent a data point, and the edges between the nodes are a measure of the similarity of those nodes. \n",
    "\n",
    "The similarity matrix in graph theory is usually an adjacency matrix $A$(weighted or not). This adjacency matrix consists in a square matrix, having as rows and columns the number of nodes $N $in the graph. The values of each matrix entry represent the edge weight, i.e.: \n",
    "\n",
    "$ A[i,j] = w_{ij}$ is the edge weight between node $i$ and node $j$.\n",
    "\n",
    "In order to construct our graph, we go back to our original dataframe before it was aggregated. We consider our nodes to be each food item, and the edges are the nutritional similarity between those nodes. \n",
    "\n",
    "We decided to compute the adjacency matrix only with the nutritional features and to leave the economic marker (i.e. median revenue) as a label. The aim of this analysis is to check if products are clustered by some \"healthiness\" measure and if so, the \"healthier\" clusters are associated with higher revenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from scipy.sparse import csgraph\n",
    "import scipy.sparse.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_adjacency(matrix_df):\n",
    "    \n",
    "    adjacency = np.zeros((matrix_df.shape[0], matrix_df.shape[0]))\n",
    "    for i in range(matrix_df.shape[1]):\n",
    "        distance = np.abs(matrix_df[:,i].reshape(-1, 1) - matrix_df[:,i])\n",
    "        distance = distance/np.max(distance)\n",
    "        \n",
    "        adjacency = adjacency + distance\n",
    "\n",
    "    adjacency = np.max(adjacency) - adjacency\n",
    "    np.fill_diagonal(adjacency, 0)\n",
    "    return adjacency/np.max(adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutri_df = dataframe.copy().dropna()\n",
    "adjacency = compute_adjacency(nutri_df[nutritional_columns].values)\n",
    "n_nodes =  len(adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = (n_nodes*n_nodes - np.count_nonzero(adjacency))*100/(n_nodes*n_nodes)\n",
    "print('Sparsity: {spars:.3f}%'.format(spars=sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our network is not sparse at all (i.e. all nodes are connected to each other) we decided to keep only the 300 strongest connections for each node for computational purposes. This makes our graph sparser and not much information is lost by the fact that it is not fully connected anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_adjacency = np.zeros(adjacency.shape)\n",
    "\n",
    "for i in range(adjacency.shape[0]):\n",
    "    thresh = np.sort(adjacency[i,:])[adjacency.shape[0] - 300 - 1]\n",
    "    idx = adjacency[i,:] > thresh\n",
    "    sparse_adjacency[i,idx] = adjacency[i,idx]\n",
    "n_nodes =  len(sparse_adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = (n_nodes*n_nodes - np.count_nonzero(sparse_adjacency))*100/(n_nodes*n_nodes)\n",
    "print('Sparsity: {spars:.3f}%'.format(spars=sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_s = nx.from_numpy_matrix(sparse_adjacency)\n",
    "laplacian_s = sparse.csgraph.laplacian(sparse_adjacency, normed=True)\n",
    "\n",
    "# We compute ordered eigenvalues e and graph Fourier basis U with eigh\n",
    "e_s , U_s = scipy.linalg.eigh(laplacian_s)\n",
    "coords_s = U_s[:,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.is_connected(G_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph is connected, yay! We can embed it using Laplacian eigenmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_sig = nutri_df['Median revenue euros'].values\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "nodes = nx.draw_networkx_nodes(G_s, coords_s, node_size=60, node_color = rev_sig)\n",
    "nx.draw_networkx_edges(G_s, coords_s, alpha=0.1)\n",
    "plt.title(\"Sparse graph with revenue as our signal\")\n",
    "plt.xlabel(\"First non-null eigenvector\")\n",
    "plt.ylabel(\"Second non-null eigenvector\")\n",
    "plt.colorbar(nodes, label='Median revenue of city where food product is sold (€)')\n",
    "plt.savefig(\"../docs/img/portfolio/graph.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try clustering with DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = cluster(coords_s, eps=2, min_samples=10)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "nodes = nx.draw_networkx_nodes(G_s, coords_s, node_size=60, node_color=labels)\n",
    "nx.draw_networkx_edges(G_s, coords_s, alpha=0.1)\n",
    "plt.title('Sparse graph with estimated labels \\n Estimated number of clusters: 1')\n",
    "plt.xlabel(\"First non-null eigenvector\")\n",
    "plt.ylabel(\"Second non-null eigenvector\")\n",
    "plt.colorbar(nodes, label='DBSCAN estimated labels')\n",
    "plt.savefig(\"../docs/img/portfolio/labels_graph.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we can find no clear cluster. Our graph is indeed fully connected, but most points are agglomerated into one big cluster, with a few food items spiking out. This is confirmed by the results of the DBSCAN clustering, which has a maximum Silhouette Coefficient when finding only one cluster. \n",
    "\n",
    "Our food products are hence extremely similar among all of them. They all have the same nutritional features and this is why cannot find more than one cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Correlation analysis \n",
    "\n",
    "In light of our clear lack of good clusters, we now want to see if there are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cols = ['nutrition-score-fr_100g', 'nutrition_grade_numeric','energy_100g', 'fat_100g', \n",
    "             'saturated-fat_100g', 'proteins_100g','fiber_100g', 'carbohydrates_100g', \n",
    "             'sugars_100g', 'cal_density', 'cal_deviation','Median revenue euros']\n",
    "corr_names = ['Nutrition score', 'Nutrition grade \\n(numeric)',\n",
    "              'Energy \\n (per 100 g)', 'Fat \\n (per 100 g)', 'Saturated fat \\n (per 100g)', \n",
    "              'Proteins \\n (per 100g)','Fiber (per 100g)', 'Carbohydrates \\n (per 100 g)', \n",
    "              'Sugars \\n(per 100g)', 'Calory density', 'Calory deviation','Median revenue (€)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_matrix(df, geography):\n",
    "    \"\"\"Display the correlation matrix for medians for a geographic area\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Dataframe containing the values of the medians for each feature, grouped by geographic area.\n",
    "    geography: string\n",
    "        Name of the type of geographic area considered.\n",
    "    \"\"\"\n",
    "    corr = np.array(df[corr_cols].corr())\n",
    "    names = corr_names\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 9))\n",
    "    im = ax.imshow(corr, vmin=-1, vmax=1, cmap='bwr', alpha=0.5)\n",
    "\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(names)))\n",
    "    ax.set_yticks(np.arange(len(names)))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(names)\n",
    "    ax.set_yticklabels(names)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    df_corr = df[corr_cols]\n",
    "    columns = df_corr.columns\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        for j in range(len(names)):\n",
    "            string = \"{0:.2f}\".format(corr[i, j])\n",
    "            if (j == 11 and i != 11):\n",
    "                r, p = stats.pearsonr(df_corr[columns[i]], df_corr['Median revenue euros'])\n",
    "                \n",
    "                if p < 0.05:\n",
    "                    text = ax.text(j, i, string,\n",
    "                                   ha=\"center\", va=\"center\", color='black', fontweight='bold')\n",
    "                else: \n",
    "                    text = ax.text(j, i, string, ha=\"center\", va=\"center\", color=\"w\")\n",
    "                        \n",
    "            else:\n",
    "                text = ax.text(j, i, string, ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    fig.colorbar(im)\n",
    "    ax.set_title(\"Correlation matrix for median aggregated values by \" + geography, y=1.03, fontweight=\"bold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_matrix(aggregated_city, \"cities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a city level, we cannot see much from the correlation matrix. The only correlations that we find are among nutritional features, e.g. the nutrition score and grade (high when unhealthy, low when healthy) is positively correlated with energy, sugars, fat and negatively correlated with fiber. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_matrix(aggregated_arrondissement, \"arrondissements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the arrondissement level, we can also mostly see the correlations between nutritional features. We can however see that the correlation between nutritional score and median revenue is negative, according to the colorbar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_matrix(aggregated_department, \"departments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the Department level (second largest subdivision of the French territory), the correlations between the nutritional features are more accentuated, especially between nutrition score and fat and saturated fat. We can see that the median revenue column (the last column) has is positively correlated to the nutrition grade, but negatively to the calory density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_corr_matrix(aggregated_region, \"regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = aggregated_region[corr_cols]\n",
    "r = np.zeros((len(df_agg.columns)))\n",
    "p = np.zeros((len(df_agg.columns)))\n",
    "\n",
    "for (i, col) in enumerate(df_agg.columns):\n",
    "    r[i], p[i] = stats.pearsonr(df_agg[col], df_agg['Median revenue euros'])\n",
    "df_corr = pd.DataFrame(np.round(r, 2), df_agg.columns, ['Median_revenue_euros'])\n",
    "df_corr.loc[:, 'p_value'] = pd.DataFrame(np.round(p,2), df_agg.columns) \n",
    "df_corr.index.name = 'nutrition_values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>nutrition_values</th>\n",
       "      <th>nutrition-score-fr_100g</th>\n",
       "      <th>nutrition_grade_numeric</th>\n",
       "      <th>energy_100g</th>\n",
       "      <th>fat_100g</th>\n",
       "      <th>saturated-fat_100g</th>\n",
       "      <th>proteins_100g</th>\n",
       "      <th>fiber_100g</th>\n",
       "      <th>carbohydrates_100g</th>\n",
       "      <th>sugars_100g</th>\n",
       "      <th>cal_density</th>\n",
       "      <th>cal_deviation</th>\n",
       "      <th>Median revenue euros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Median_revenue_euros</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "nutrition_values      nutrition-score-fr_100g  nutrition_grade_numeric  \\\n",
       "Median_revenue_euros                     0.26                     0.67   \n",
       "\n",
       "nutrition_values      energy_100g  fat_100g  saturated-fat_100g  \\\n",
       "Median_revenue_euros         0.18     -0.03                 0.2   \n",
       "\n",
       "nutrition_values      proteins_100g  fiber_100g  carbohydrates_100g  \\\n",
       "Median_revenue_euros           0.33       -0.37                0.09   \n",
       "\n",
       "nutrition_values      sugars_100g  cal_density  cal_deviation  \\\n",
       "Median_revenue_euros         0.17        -0.75           0.37   \n",
       "\n",
       "nutrition_values      Median revenue euros  \n",
       "Median_revenue_euros                   1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_corr.Median_revenue_euros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_corr.Median_revenue_euros)\n",
    "\n",
    "colors = ['#ca0020','#f4a582','#f7f7f7', '#f7f7f7', '#92c5de','#0571b0']\n",
    "mapper = LinearColorMapper(palette=colors, low=-1, high=1)\n",
    "\n",
    "TOOLS = \"hover,save,pan,box_zoom,reset,wheel_zoom\"\n",
    "\n",
    "p = figure(title=\"Correlation between nutrition \\n values and Region revenue in €\",\n",
    "           x_range=list(df.columns),\n",
    "           y_range=list(reversed(df.index)),\n",
    "           x_axis_location=\"above\", plot_width=300, plot_height=800,\n",
    "           tools=TOOLS, toolbar_location='right')\n",
    "\n",
    "p.grid.grid_line_color = None\n",
    "p.axis.axis_line_color = None\n",
    "p.axis.major_tick_line_color = None\n",
    "p.axis.major_label_text_font_size = \"8pt\"\n",
    "p.axis.major_label_standoff = 0\n",
    "p.xaxis.major_label_orientation = 0\n",
    "\n",
    "hover = p.select_one(HoverTool)\n",
    "hover.point_policy = 'follow_mouse'\n",
    "hover.tooltips = [('Correlation', '@Median_revenue_euros{ 0.02}')]\n",
    "\n",
    "p.rect(x=\"Median_revenue_euros\", y=\"nutrition_values\", width=8, height=1,\n",
    "       source=df,\n",
    "       fill_color={'field': 'Median_revenue_euros', 'transform': mapper},\n",
    "       line_color=None)\n",
    "\n",
    "color_bar = ColorBar(color_mapper=mapper, major_label_text_font_size=\"8pt\",\n",
    "                     ticker=BasicTicker(desired_num_ticks=len(colors)),\n",
    "                     label_standoff=6, border_line_color=None, location=(0, 0))\n",
    "p.add_layout(color_bar, 'right')\n",
    "\n",
    "show(p)      # show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.layouts import gridplot\n",
    "\n",
    "TOOLS = \"hover,save,pan,box_zoom,reset,wheel_zoom\"\n",
    "\n",
    "#p1 = figure(title=\"p-value corresponding to each nutrition value\",\n",
    "#            tools=TOOLS, x_axis_location=None, y_axis_location=None)\n",
    "p1 = figure(title=\"p_value of the different pearson coefficients\",\n",
    "           x_range=list(df_corr.index),\n",
    "           x_axis_location=\"below\", plot_width=1400, plot_height=800,\n",
    "           tools=TOOLS, toolbar_location='right')\n",
    "p1.grid.grid_line_alpha=0.3\n",
    "p1.xaxis.axis_label = 'nutrition value'\n",
    "p1.yaxis.axis_label = 'p-value'\n",
    "\n",
    "hover = p1.select_one(HoverTool)\n",
    "hover.point_policy = 'follow_mouse'\n",
    "hover.mode = 'vline'\n",
    "hover.tooltips = [('p_value:', '@p_value')]\n",
    "\n",
    "p1.line(range(0,len(df_corr.index)), df_corr.p_value,  color='#FB9A99', legend='Pearson coefficient p-value')\n",
    "p1.legend.location = \"top_right\"\n",
    "\n",
    "#p1.line(range(0,len(df_corr.index)), 0.05, color='black', legend='alpha = 0.05', line_dash='dashed')\n",
    "#p1.legend.location = \"top_right\"\n",
    "\n",
    "output_file(\"p_value_regions.html\", title=\"p_value_regions\")\n",
    "\n",
    "show(gridplot([[p1]]))  # open a browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_corr.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.layouts import row\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "\n",
    "factors = df_corr.index.values\n",
    "x =  df_corr.p_value.values\n",
    "\n",
    "dot = figure(title=\"Categorical Dot Plot\", tools=\"hover\", toolbar_location=None,\n",
    "            y_range=factors, x_range=[0,1])\n",
    "\n",
    "hover = dot.select_one(HoverTool)\n",
    "hover.point_policy = 'follow_mouse'\n",
    "hover.tooltips = [('p_value:', '@p_value')]\n",
    "\n",
    "dot.segment(0, factors, x, factors, line_width=2, line_color=\"green\", )\n",
    "dot.line(y=range(0,len(df_corr.index)), x=0.05, color='black', legend='alpha = 0.05', line_dash='dashed')\n",
    "dot.yaxis.axis_label = 'nutrition value'\n",
    "dot.xaxis.axis_label = 'p-value'\n",
    "\n",
    "df = pd.DataFrame(df_corr.Median_revenue_euros)\n",
    "\n",
    "colors = ['#ca0020','#f4a582','#f7f7f7', '#f7f7f7', '#92c5de','#0571b0']\n",
    "mapper = LinearColorMapper(palette=colors, low=-1, high=1)\n",
    "\n",
    "TOOLS = \"hover,save,pan,box_zoom,reset,wheel_zoom\"\n",
    "\n",
    "p = figure(title=\"Correlation between nutrition \\n values and Region revenue in €\",\n",
    "           x_range=list(df.columns),\n",
    "           y_range=list(df.index),\n",
    "           x_axis_location=\"above\", plot_width=300, plot_height=800,\n",
    "           tools=TOOLS, toolbar_location=None)\n",
    "\n",
    "p.axis.major_label_text_font_size = \"8pt\"\n",
    "\n",
    "hover = p.select_one(HoverTool)\n",
    "hover.point_policy = 'follow_mouse'\n",
    "hover.tooltips = [('Correlation', '@Median_revenue_euros{ 0.02}')]\n",
    "\n",
    "p.rect(x=\"Median_revenue_euros\", y=\"nutrition_values\", width=8, height=1,\n",
    "       source=df,\n",
    "       fill_color={'field': 'Median_revenue_euros', 'transform': mapper},\n",
    "       line_color=None)\n",
    "\n",
    "color_bar = ColorBar(color_mapper=mapper, major_label_text_font_size=\"8pt\",\n",
    "                     ticker=BasicTicker(desired_num_ticks=len(colors)),\n",
    "                     label_standoff=6, border_line_color=None, location=(0, 0))\n",
    "p.add_layout(color_bar, 'right')\n",
    "\n",
    "output_file(\"categorical.html\", title=\"categorical.py example\")\n",
    "\n",
    "show(row(p, dot, sizing_mode=\"scale_height\"))  # open a browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray, we found something at the arrondissement and region levels!\n",
    "\n",
    "Now we can see that the Median revenue column (the last column) has a strong negative correlation with most of the nutritional features: nutrition score, nutrition grade, serving size, energy, fat, saturated fat, sugars, carbohydrates and calory density. This might indicate that the poorer the region, the worse the quality of the available products. This is reinforced by the fact that the Median revenue is positively correlated with the proteins and fiber (richer regions have more available products that have proteins and fibers).\n",
    "\n",
    "Let's see more closely how significant these correlations are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_pval_df(df):\n",
    "    \"\"\"Compute and display the Pearson coefficient and the p-value between the median revenue\n",
    "    and each nutritional feature\n",
    "    \n",
    "    A line displayed in red means that the correlation is not significant.\n",
    "    A line displayed in green means that the  correlation is significant.\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Dataframe containing the values for each feature, aggregated by geographic area.\n",
    "    \"\"\"\n",
    "    print('Linear correlation between Median revenue and each of the nutritional columns')\n",
    "    for col in nutritional_columns:\n",
    "        r, p = stats.pearsonr(df[col], df['Median revenue euros'])\n",
    "        if p < 0.05:\n",
    "            color = 'green'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        print(colored('{:<40}{:<16}{:<28.3f}{:<10}{:<30.3f}'.format(col, 'Pearson coeff', r, 'p-value', p), color))\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_pval_df(aggregated_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the mean dataframe, the columns for which the correlation with the Median revenue is significant (i.e. the corresponding p-value is below 0.05) are:\n",
    "      \n",
    "* nutrition_grade_numeric\n",
    "* cal_density          \n",
    "\n",
    "We have a negative correlation between the calory density column and the mean median revenue for the overall regions. The Pearson coefficient is below the -0.3 threshold , which allows us to claim that there is a correlation. It is actually a very significant correlation since the coefficient is strongly negative (-0.748).\n",
    "This might indicate that in poor regions, the food items that are available might have more calories per serving.\n",
    "\n",
    "However, we also have a very significant (p-value = 0.017) **positive** correlation between the Nutrition Grade and the Median revenue. Since the nutrition grade is higher for unhealthier products, this could indicate that wealthier regions have unhealthier available products. \n",
    "\n",
    "A closer look to this metric shows us that the correlation is meaningless. Indeed, the median nutrition grade for each region is **constantly 3 except for one where it is 2**. This result is hence not exploitable.\n",
    "\n",
    "Here is a scatter plot showing the most significant correlation that we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fig = sns.jointplot(x=aggregated_region['Median revenue euros'], y=aggregated_region['cal_density'], kind=\"reg\", color='b')\n",
    "fig.fig.suptitle(\"\"\"Median revenue vs calory density at a region level\"\"\", y=1.01,\n",
    "                 fontweight=\"bold\"\n",
    "                )\n",
    "plt.xlabel(\"Median revenue (€)\")\n",
    "plt.ylabel(\"Calory density (kcal per seving size)\")\n",
    "plt.savefig('../docs/img/portfolio/corr_thumb.pdf', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pearson_pval_df(aggregated_arrondissement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At an arrondissement level, we find a negative relationship between energy and median revenue. It is the only relationship that is significant since its p-value falls below the 0.05 threshold.\n",
    "\n",
    "This might indicate that poor regions could be prone to buying products that are more caloric than wealthier regions, which goes in the direction of our thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fig = sns.jointplot(x=aggregated_arrondissement['Median revenue euros'], y=aggregated_arrondissement['energy_100g'], kind=\"reg\", color='b')\n",
    "fig.fig.suptitle(\"\"\"Median revenue vs energy at a region level\"\"\",\n",
    "                 y=1.05,\n",
    "                 fontweight=\"bold\"\n",
    "                )\n",
    "plt.xlabel(\"Median revenue (€)\")\n",
    "plt.ylabel(\"Energy per 100g of product (kcal)\")\n",
    "plt.savefig(\"../docs/img/portfolio/corr_thumb.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Geographic visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make Folium maps, we need to reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_arrondissement_med = aggregated_arrondissement.reset_index()\n",
    "aggregated_department_med = aggregated_department.reset_index()\n",
    "aggregated_region_med = aggregated_region.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an auxiliary function that creates an overlay and deals with missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_choropleth(df, col_property, geojson, territory, legend_name, fill_color='OrRd', legend_step=6):\n",
    "    map_data = df.copy()\n",
    "\n",
    "    map_dict = map_data.set_index(territory)[col_property].to_dict()\n",
    "    \n",
    "    min_data = np.min(map_data[col_property])\n",
    "    max_data = np.max(map_data[col_property])\n",
    "    scale = list(np.linspace(min_data, max_data, legend_step))\n",
    "    color_range = color_brewer(fill_color, n=legend_step-1)\n",
    "\n",
    "    color_scale = StepColormap(color_range, vmin=min_data, vmax=max_data, index=scale, caption=legend_name)\n",
    "\n",
    "    def get_color(feature):\n",
    "        value = map_dict.get(feature['properties']['code'])\n",
    "        if value is None:\n",
    "            return '#8c8c8c' # MISSING -> gray\n",
    "        else:\n",
    "            return color_scale(value)\n",
    "\n",
    "    m = folium.Map(location=[46.5, 2.3], tiles='cartodbpositron', zoom_start=6)\n",
    "\n",
    "    folium.GeoJson(\n",
    "        data = geojson,\n",
    "        style_function = lambda feature: {\n",
    "            'fillColor': get_color(feature),\n",
    "            'fillOpacity': 0.7,\n",
    "            'color' : 'black',\n",
    "            'weight' : 0.5,  \n",
    "        }    \n",
    "    ).add_to(m)\n",
    "    \n",
    "    m.add_child(color_scale)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geojson: https://github.com/gregoiredavid/france-geojson\n",
    "arr_borders = json.load(open(\"../data/raw/arrondissements.geojson\"))\n",
    "reg_borders = json.load(open(\"../data/raw/regions.geojson\"))\n",
    "dep_borders = json.load(open(\"../data/raw/departements.geojson\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Arrondissements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_revenue_arr = other_choropleth(aggregated_arrondissement_med, 'Median revenue euros', \n",
    "                                   arr_borders, 'Arrondissement', 'Median revenue (€)', fill_color='OrRd_r')\n",
    "map_revenue_arr.save(\"../data/processed/map_{}.html\".format('med_rev_arr'))\n",
    "display(HTML(\"<a href='../data/processed/map_{0}.html' target='_blank'>{0}</a>\".format('med_rev_arr')))\n",
    "map_revenue_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_energy_arr = other_choropleth(aggregated_arrondissement_med, 'energy_100g', arr_borders, 'Arrondissement', 'Energy per 100 g (in kJ)')\n",
    "map_energy_arr.save(\"../data/processed/map_{}.html\".format('med_energy_arr'))\n",
    "display(HTML(\"<a href='../data/processed/map_{0}.html' target='_blank'>{0}</a>\".format('med_energy_arr')))\n",
    "map_energy_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Regions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_revenue_reg = other_choropleth(aggregated_region_med, 'Median revenue euros', reg_borders, 'Region', 'Median revenue (€)', fill_color='OrRd_r',legend_step=6)\n",
    "map_revenue_reg.save(\"../data/processed/map_{}.html\".format('med_rev_reg'))\n",
    "display(HTML(\"<a href='../data/processed/map_{0}.html' target='_blank'>{0}</a>\".format('med_rev_reg')))\n",
    "map_revenue_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "map_calden_reg = other_choropleth(aggregated_region_med, 'cal_density', reg_borders, 'Region', 'Calory density', legend_step=6)\n",
    "map_calden_reg.save(\"../data/processed/map_{}.html\".format('med_cd_reg'))\n",
    "display(HTML(\"<a href='../data/processed/map_{0}.html' target='_blank'>{0}</a>\".format('med_cd_reg')))\n",
    "map_calden_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_nutri_reg = other_choropleth(aggregated_region_med, 'nutrition_grade_numeric', reg_borders, 'Region', 'Nutrition grade', legend_step=6)\n",
    "map_nutri_reg.save(\"../data/processed/map_{}.html\".format('med_nutri_reg'))\n",
    "display(HTML(\"<a href='../data/processed/map_{0}.html' target='_blank'>{0}</a>\".format('med_nutri_reg')))\n",
    "map_nutri_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Interactive Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import OrRd\n",
    "from bokeh.plotting import figure, save, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, BasicTicker, \\\n",
    "                         ColorBar, CustomJS, Slider, Toggle, Select\n",
    "import geopandas as gpd\n",
    "import pysal as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fiona\n",
    "import numpy as np\n",
    "from bokeh.io import show, output_file\n",
    "from bokeh.palettes import Reds6 as palette\n",
    "from bokeh.resources import CDN\n",
    "from shapely.geometry import Polygon, Point, MultiPoint, MultiPolygon\n",
    "from shapely.prepared import prep\n",
    "from bokeh.layouts import column, row, widgetbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXYCoords(geometry, coord_type):\n",
    "    \"\"\" Returns either x or y coordinates from  geometry coordinate sequence. Used with LineString and Polygon geometries.\"\"\"\n",
    "    if coord_type == 'x':\n",
    "        return [x[0] for x in geometry]\n",
    "    elif coord_type == 'y':\n",
    "        return [x[1] for x in geometry]\n",
    "\n",
    "def getPolyCoords(geometry, coord_type):\n",
    "    \"\"\" Returns Coordinates of Polygon using the Exterior of the Polygon.\"\"\"\n",
    "    ext = geometry['coordinates'][0]\n",
    "    return getXYCoords(ext, coord_type)\n",
    "\n",
    "def getLineCoords(geometry, coord_type):\n",
    "    \"\"\" Returns Coordinates of Linestring object.\"\"\"\n",
    "    return getXYCoords(geometry, coord_type)\n",
    "\n",
    "def getPointCoords(geometry, coord_type):\n",
    "    \"\"\" Returns Coordinates of Point object.\"\"\"\n",
    "    if coord_type == 'x':\n",
    "        return geometry[0]\n",
    "    elif coord_type == 'y':\n",
    "        return geometry[1]\n",
    "\n",
    "def multiGeomHandler(multi_geometry, coord_type, geom_type):\n",
    "    \"\"\"\n",
    "    Function for handling multi-geometries. Can be MultiPoint, MultiLineString or MultiPolygon.\n",
    "    Returns a list of coordinates where all parts of Multi-geometries are merged into a single list.\n",
    "    Individual geometries are separated with np.nan which is how Bokeh wants them.\n",
    "    # Bokeh documentation regarding the Multi-geometry issues can be found here (it is an open issue)\n",
    "    # https://github.com/bokeh/bokeh/issues/2321\n",
    "    \"\"\"\n",
    "\n",
    "    for i, part in enumerate(multi_geometry[\"coordinates\"]):\n",
    "        # On the first part of the Multi-geometry initialize the coord_array (np.array)\n",
    "        if i == 0:\n",
    "            coord_arrays = getXYCoords(part[0], coord_type)\n",
    "            max_length = len(part[0])\n",
    "        else:\n",
    "            if len(part[0]) > max_length:\n",
    "                max_length = len(part[0])\n",
    "                coord_arrays = getXYCoords(part[0], coord_type)\n",
    "    # Return the coordinates\n",
    "    return coord_arrays\n",
    "\n",
    "\n",
    "def getCoords(row, geom_col, coord_type):\n",
    "    \"\"\"\n",
    "    Returns coordinates ('x' or 'y') of a geometry (Point, LineString or Polygon) as a list (if geometry is LineString or Polygon).\n",
    "    Can handle also MultiGeometries.\n",
    "    \"\"\"\n",
    "    # Get geometry\n",
    "    geom = row[geom_col]\n",
    "\n",
    "    # Check the geometry type\n",
    "    gtype = geom[\"type\"]\n",
    "\n",
    "    # \"Normal\" geometries\n",
    "    # -------------------\n",
    "\n",
    "    if gtype == \"Point\":\n",
    "        return getPointCoords(geom, coord_type)\n",
    "    elif gtype == \"LineString\":\n",
    "        return list( getLineCoords(geom, coord_type) )\n",
    "    elif gtype == \"Polygon\":\n",
    "        return list( getPolyCoords(geom, coord_type) )\n",
    "\n",
    "    # Multi geometries\n",
    "    # ----------------\n",
    "\n",
    "    else:\n",
    "        return list( multiGeomHandler(geom, coord_type, gtype) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_args_for_maps(area_name, column_name):\n",
    "    geojson_file_name = \"../data/raw/{}s.geojson\".format(area_name.lower())\n",
    "    # create dataframe\n",
    "    data = gpd.read_file(geojson_file_name, encoding='utf-8')\n",
    "    df = aggregate(dataframe, column_name)\n",
    "    df.reset_index(inplace=True)\n",
    "    joined_df = data.merge(df, left_on=\"code\", right_on=column_name, how=\"left\")\n",
    "    joined_df.drop(columns={\"geometry\", \"code\", \"food_item_index\", column_name}, inplace=True)\n",
    "    \n",
    "    # get x and y for contour of the areas\n",
    "    borders = json.load(open(geojson_file_name))\n",
    "    areas_x = [getCoords(i, geom_col=\"geometry\", coord_type=\"x\") for i in borders['features']]\n",
    "    areas_y = [getCoords(i, geom_col=\"geometry\", coord_type=\"y\") for i in borders['features']]\n",
    "    \n",
    "    # create dict for arguments\n",
    "    arguments = {\"{}_x\".format(column_name.lower()): areas_x, \"{}_y\".format(column_name.lower()): areas_y,}\n",
    "    for column in joined_df.columns:\n",
    "         arguments[\"{}_{}\".format(column_name.lower(), column)] = joined_df[column].values\n",
    "    return arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasources_for_map():\n",
    "    datasources = {}\n",
    "    for area_name, column_name in [(\"Arrondissement\", \"Arrondissement\"), \n",
    "                                   (\"Departement\", \"Department\"), \n",
    "                                   (\"Region\", \"Region\")\n",
    "                                  ]:\n",
    "        args = create_args_for_maps(area_name, column_name)\n",
    "        args['x'] = args['{}_x'.format(column_name.lower())]\n",
    "        args['y'] = args['{}_y'.format(column_name.lower())]\n",
    "        args['name'] = args['{}_nom'.format(column_name.lower())]\n",
    "        datasources[column_name.lower()] = ColumnDataSource(args)\n",
    "    return datasources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_update_values(column_name, display_name):\n",
    "    return \"\"\"\n",
    "        {column_name} = cb_obj.value;\n",
    "        for(let option of cb_obj.options) {{\n",
    "            if(option[0] == {column_name}) {{\n",
    "                {display_name} = option[1]\n",
    "            }}\n",
    "        }}\n",
    "    \"\"\".format(column_name=column_name, display_name=display_name)\n",
    "\n",
    "def get_code_update_callback(callback_name, mapping):\n",
    "    s = \"\"\n",
    "    for key, value in mapping.items():\n",
    "        s += \"{callback_name}.args['{key}'] = {value};\".format(callback_name=callback_name, key=key, value=value)\n",
    "    return s\n",
    "\n",
    "def color_helpers(low, high):\n",
    "    color_mapper = LinearColorMapper(palette=OrRd[5], low=low, high=high)\n",
    "    color_bar = ColorBar(color_mapper=color_mapper, ticker=BasicTicker(), location=(0, 0))\n",
    "    return color_mapper, color_bar\n",
    "\n",
    "def create_map_figure(source, color_mapper, color_bar):\n",
    "    TOOLS = \"pan,wheel_zoom,hover\"\n",
    "    p = figure(\n",
    "        title=\"Geographical distribution of the nutrition grade\", tools=TOOLS,\n",
    "        x_axis_location=None, y_axis_location=None\n",
    "    )\n",
    "    p.grid.grid_line_color = None\n",
    "    p.patches('x', 'y', source=source,\n",
    "              fill_color={'field': 'display', 'transform': color_mapper},\n",
    "              fill_alpha=0.8, line_color=\"black\", line_width=0.3)\n",
    "    \n",
    "    p.add_layout(color_bar, 'right')\n",
    "    return p\n",
    "\n",
    "def define_hover(p, area_name, display_name):\n",
    "    hover = p.select_one(HoverTool)\n",
    "    hover.point_policy = \"follow_mouse\"\n",
    "    hover.tooltips = [(area_name, \"@name\"),(display_name, \"@display\"), (\"(Long, Lat)\", \"($x, $y)\")]\n",
    "    return hover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_update_hover(hover_name, value_variable_name):\n",
    "    return \"\"\"\n",
    "    {hover_name}.tooltips = [\n",
    "        [area_name, \"@name\"], \n",
    "        [{value_variable_name}, \"@display\"], \n",
    "        [\"(Long, Lat)\", \"($x, $y)\"]]\n",
    "    \"\"\".format(hover_name=hover_name, value_variable_name=value_variable_name)\n",
    "\n",
    "def get_code_update_cmap(cmap_name, details_name):\n",
    "    return \"\"\"\n",
    "        {cmap_name}.low = Math.min(...{details_name}.filter(x => !isNaN(x)))\n",
    "        {cmap_name}.high = Math.max(...{details_name}.filter(x => !isNaN(x)))\n",
    "    \"\"\".format(cmap_name=cmap_name, details_name=details_name)\n",
    "\n",
    "def get_code_update_map_from_area(map_index):\n",
    "    return \"\"\"\n",
    "        let column_name{map_index} = `${{area_column}}_${{value_map{map_index}_column}}`\n",
    "        {update_hover}\n",
    "        let details_map{map_index} = sources[area_column].data[column_name{map_index}]\n",
    "        {update_cmap}\n",
    "        source_map{map_index}.data['x'] = sources[area_column].data[`${{area_column}}_x`]\n",
    "        source_map{map_index}.data['y'] = sources[area_column].data[`${{area_column}}_y`]\n",
    "        source_map{map_index}.data['name'] = sources[area_column].data[`${{area_column}}_nom`]\n",
    "        source_map{map_index}.data['display'] = details_map{map_index}\n",
    "        {update_values_callback}\n",
    "        source_map{map_index}.change.emit();\n",
    "    \"\"\".format(\n",
    "        map_index=map_index,\n",
    "        update_hover=get_code_update_hover(\n",
    "            \"hover_map{map_index}\".format(map_index=map_index), \n",
    "            \"value_map{map_index}_name\".format(map_index=map_index)),\n",
    "        update_cmap=get_code_update_cmap(\n",
    "            \"cmap_map{map_index}\".format(map_index=map_index),\n",
    "            \"details_map{map_index}\".format(map_index=map_index)\n",
    "        ),\n",
    "        update_values_callback=get_code_update_callback(\n",
    "            \"callback_values_map{map_index}\".format(map_index=map_index),\n",
    "            {\n",
    "                'hover': 'hover_map{map_index}'.format(map_index=map_index),\n",
    "                'cmap': 'cmap_map{map_index}'.format(map_index=map_index),\n",
    "                'value_name': 'value_map{map_index}_name'.format(map_index=map_index),\n",
    "                'value_column': 'value_map{map_index}_column'.format(map_index=map_index),\n",
    "                'area_name': 'area_name',\n",
    "                'area_column': 'area_column',\n",
    "                'source': 'source_map{map_index}'.format(map_index=map_index),\n",
    "                'map_figure': 'map{map_index}'.format(map_index=map_index),\n",
    "            }\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_area_selector(options, initial_value, js_args):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # define callback\n",
    "    callback_area = CustomJS(args=js_args, code=\"\"\"\n",
    "             {update_value}\n",
    "             {update_map1}\n",
    "             {update_map2}\n",
    "             {update_local_callback}\n",
    "            \"\"\".format(\n",
    "                    update_value=get_code_update_values(\"area_column\", \"area_name\"),\n",
    "                    update_map1=get_code_update_map_from_area(map_index=1),\n",
    "                    update_map2=get_code_update_map_from_area(map_index=2),\n",
    "                    update_local_callback=get_code_update_callback(\"this.callback\",\n",
    "                                                                   {\n",
    "                                                                      'hover_map1': 'hover_map1',\n",
    "                                                                      'cmap_map1': 'cmap_map1',\n",
    "                                                                      'value_map1_name': 'value_map1_name',\n",
    "                                                                      'value_map1_column': 'value_map1_column',\n",
    "                                                                      'source_map1': 'source_map1',\n",
    "                                                                      'map1': 'map1',\n",
    "                                                                      'area_name': 'area_name',\n",
    "                                                                      'area_column': 'area_column',\n",
    "                                                                      'hover_map2': 'hover_map2',\n",
    "                                                                      'cmap_map2': 'cmap_map2',\n",
    "                                                                      'value_map2_name': 'value_map2_name',\n",
    "                                                                      'value_map2_column': 'value_map2_column',\n",
    "                                                                      'source_map2': 'source_map2',\n",
    "                                                                      'map2': 'map2',\n",
    "                                                                  })\n",
    "            ))\n",
    "    # define element\n",
    "    area_selection = Select(title=\"Area:\", value=initial_value,\n",
    "                             options=options,\n",
    "                             callback=callback_area,\n",
    "                            )\n",
    "    return widgetbox(area_selection), callback_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map(sources, initial_values, options, map_index):\n",
    "    # add column 'display' to source\n",
    "    initial_source = ColumnDataSource(sources[initial_values[\"area_column\"]].data)\n",
    "    initial_source.data['display'] = sources[initial_values[\"area_column\"]].data[\n",
    "        \"{}_{}\".format(initial_values[\"area_column\"], initial_values[\"value_column\"])\n",
    "    ]\n",
    "    \n",
    "    # color handling\n",
    "    cmap, color_bar = color_helpers(initial_values['low'], initial_values['high'])\n",
    "\n",
    "    map_figure = create_map_figure(initial_source, cmap, color_bar)\n",
    "\n",
    "    # hover behavior\n",
    "    hover = define_hover(map_figure, initial_values['area_name'], initial_values['value_name'])\n",
    "    \n",
    "    # define callback\n",
    "    callback_values = CustomJS(args=dict(map_figure=map_figure, hover=hover, cmap=cmap, \n",
    "                                         source=initial_source, sources=sources, **initial_values), \n",
    "                               code=\"\"\"\n",
    "        {update_value}\n",
    "        let column_name = `${{area_column}}_${{value_column}}`\n",
    "        map_figure.title.text = `Geographical distribution of the ${{value_name}}`;\n",
    "        {update_hover}\n",
    "        let details = sources[area_column].data[column_name]\n",
    "        {update_cmap}\n",
    "        source.data['display'] = details;\n",
    "        {update_area_callback}\n",
    "        {update_local_callback}\n",
    "        let options = available_options.filter(x => x[0] != value_selection_self.value)\n",
    "        value_selection_other.options = options\n",
    "        source.change.emit();\n",
    "        \"\"\".format(\n",
    "            update_value=get_code_update_values(\"value_column\", \"value_name\"),\n",
    "            update_hover=get_code_update_hover(\"hover\", \"value_name\"),\n",
    "            update_cmap=get_code_update_cmap(\"cmap\", \"details\"),\n",
    "            update_area_callback=get_code_update_callback(\"callback_areas\", \n",
    "                                                          {\n",
    "                                                              'hover_map{}'.format(map_index): 'hover',\n",
    "                                                              'cmap_map{}'.format(map_index): 'cmap',\n",
    "                                                              'value_map{}_name'.format(map_index): 'value_name',\n",
    "                                                              'value_map{}_column'.format(map_index): 'value_column',\n",
    "                                                              'area_name': 'area_name',\n",
    "                                                              'area_column': 'area_column',\n",
    "                                                              'source_map{}'.format(map_index): 'source',\n",
    "                                                              'map{}'.format(map_index): 'map_figure',\n",
    "                                                          }),\n",
    "            update_local_callback=get_code_update_callback(\"this.callback\",\n",
    "                                                          {\n",
    "                                                              'hover': 'hover',\n",
    "                                                              'cmap': 'cmap',\n",
    "                                                              'value_name': 'value_name',\n",
    "                                                              'value_column': 'value_column',\n",
    "                                                              'area_name': 'area_name',\n",
    "                                                              'area_column': 'area_column',\n",
    "                                                              'source': 'source',\n",
    "                                                              'map_figure': 'map_figure',\n",
    "                                                          })\n",
    "        ))\n",
    "    value_selection = Select(title=\"Value:\", value=initial_values[\"value_column\"],\n",
    "                         options=options,\n",
    "                         callback=callback_values,\n",
    "                        )\n",
    "    callback_values.args['value_selection_self'] = value_selection\n",
    "    return map_figure, hover, cmap, initial_source, callback_values, value_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_map_comparison(initial_area_name, initial_area_column, initial_values_map1, \n",
    "                           initial_values_map2, options_values, options_areas):\n",
    "    # initial values\n",
    "    initial_values_map1['area_name'] = initial_area_name\n",
    "    initial_values_map1['area_column'] = initial_area_column\n",
    "    \n",
    "    initial_values_map2['area_name'] = initial_area_name\n",
    "    initial_values_map2['area_column'] = initial_area_column\n",
    "\n",
    "    map1, hover_map1, cmap_map1, source_map1, callback_values_map1, value_selection_map1 = create_map(\n",
    "        sources=sources, initial_values=initial_values_map1, \n",
    "        options=list(filter(lambda x: x[0] != initial_values_map2['value_column'], options_values)),\n",
    "        map_index=1\n",
    "    )\n",
    "    map2, hover_map2, cmap_map2, source_map2, callback_values_map2, value_selection_map2 = create_map(\n",
    "        sources=sources, initial_values=initial_values_map2, \n",
    "        options=list(filter(lambda x: x[0] != initial_values_map1['value_column'], options_values)),\n",
    "        map_index=2\n",
    "    )\n",
    "    area_selection, callback_area = create_area_selector(options=options_areas, \n",
    "                                                          initial_value=initial_area_column,\n",
    "                                                          js_args=dict(map1=map1, \n",
    "                                                            hover_map1=hover_map1, cmap_map1=cmap_map1, \n",
    "                                                            source_map1=source_map1,\n",
    "                                                            value_map1_name=initial_values_map1['value_name'], \n",
    "                                                            value_map1_column=initial_values_map1['value_column'],\n",
    "                                                            map2=map2, \n",
    "                                                            hover_map2=hover_map2, cmap_map2=cmap_map2, \n",
    "                                                            source_map2=source_map2,\n",
    "                                                            value_map2_name=initial_values_map2['value_name'], \n",
    "                                                            value_map2_column=initial_values_map2['value_column'],\n",
    "                                                            sources=sources,\n",
    "                                                            area_name=initial_area_name, \n",
    "                                                            area_column=initial_area_column\n",
    "                                                           ))\n",
    "    callback_values_map1.args['callback_areas'] = callback_area\n",
    "    callback_values_map2.args['callback_areas'] = callback_area\n",
    "    callback_values_map1.args['available_options'] = options_values\n",
    "    callback_values_map2.args['available_options'] = options_values\n",
    "    callback_values_map1.args['value_selection_other'] = value_selection_map2\n",
    "    callback_values_map2.args['value_selection_other'] = value_selection_map1\n",
    "    callback_area.args['callback_values_map1'] = callback_values_map1\n",
    "    callback_area.args['callback_values_map2'] = callback_values_map2\n",
    "\n",
    "    # display map\n",
    "    show(\n",
    "        column(\n",
    "            area_selection,\n",
    "            row(\n",
    "                column(\n",
    "                    widgetbox(value_selection_map1),\n",
    "                    map1,\n",
    "                ),\n",
    "                column(\n",
    "                    widgetbox(value_selection_map2),\n",
    "                    map2,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = create_datasources_for_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_map_comparison(\"Department\", \"department\", \n",
    "                       dict(low=np.min(aggregated_department_med['nutrition_grade_numeric']),\n",
    "                            high=np.max(aggregated_department_med['nutrition_grade_numeric']),\n",
    "                            value_name=\"Nutrition grade\",\n",
    "                            value_column=\"nutrition_grade_numeric\",\n",
    "                        ),\n",
    "                        dict(low=np.min(aggregated_department_med['Median revenue euros']),\n",
    "                             high=np.max(aggregated_department_med['Median revenue euros']),\n",
    "                             value_name=\"Median revenue in €\",\n",
    "                             value_column=\"Median revenue euros\",\n",
    "                        ),\n",
    "                        [\n",
    "                            (\"Median revenue euros\", \"Median revenue\"), \n",
    "                            (\"nutrition_grade_numeric\", \"Nutrition grade\"),\n",
    "                            ('serving_size', 'Serving size'),\n",
    "                        ],\n",
    "                        [\n",
    "                            (\"region\", \"Region\"), \n",
    "                            (\"department\", \"Department\"), \n",
    "                            ('arrondissement', \"Arrondissement\")\n",
    "                        ]\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
